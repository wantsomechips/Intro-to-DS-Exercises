{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science 2025\n",
    "\n",
    "# Week 6: Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 | Linear regression with feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the [TED Talks](https://www.kaggle.com/rounakbanik/ted-talks) dataset from Kaggle. Your task is to predict both the ratings and the number of views of a given TED talk. You should focus only on the <span style=\"font-weight: bold\">ted_main</span> table.\n",
    "\n",
    "1. Download the data, extract the following ratings from column <span style=\"font-weight: bold\">ratings</span>: <span style=\"font-weight: bold\">Funny</span>, <span style=\"font-weight: bold\">Confusing</span>, <span style=\"font-weight: bold\">Inspiring</span>. Store these values into respective columns so that they are easier to access. Next, extract the tags from column <span style=\"font-weight: bold\">tags</span>. Count the number of occurrences of each tag and select the top-100 most common tags. Create a binary variable for each of these and include them in your data table, so that you can directly see whether a given tag (among the top-100 tags) is used in a given TED talk or not. The dataset you compose should have dimension (2550, 104), and comprise of the 'views' column, the three columns with counts of \"Funny\", \"Confusing and \"Inspiring\" ratings, and 100 columns which one-hot encode the top-100 most common tag columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport ast\nfrom collections import Counter\n\ndf = pd.read_csv('ted_main.csv')\n\ndef extract_rating(ratings_str, rating_name):\n    ratings_list = ast.literal_eval(ratings_str)\n    for rating in ratings_list:\n        if rating['name'] == rating_name:\n            return rating['count']\n    return 0\n\ndf['Funny'] = df['ratings'].apply(lambda x: extract_rating(x, 'Funny'))\ndf['Confusing'] = df['ratings'].apply(lambda x: extract_rating(x, 'Confusing'))\ndf['Inspiring'] = df['ratings'].apply(lambda x: extract_rating(x, 'Inspiring'))\n\nall_tags = []\nfor tags_str in df['tags']:\n    tags_list = ast.literal_eval(tags_str)\n    all_tags.extend(tags_list)\n\ntag_counts = Counter(all_tags)\ntop_100_tags = [tag for tag, count in tag_counts.most_common(100)]\n\nfor tag in top_100_tags:\n    df[f'tag_{tag}'] = df['tags'].apply(lambda x: 1 if tag in ast.literal_eval(x) else 0)\n\ndata = df[['views', 'Funny', 'Confusing', 'Inspiring'] + [f'tag_{tag}' for tag in top_100_tags]]\n\nprint(f\"Dataset shape: {data.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Construct a linear regression model to predict the number of views based on the data in the <span style=\"font-weight: bold\">ted_main</span> table, including the binary variables for the top-100 tags that you just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error\n\nX = data.drop('views', axis=1)\ny = data['views']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel_views = LinearRegression()\nmodel_views.fit(X_train, y_train)\n\ny_pred = model_views.predict(X_test)\nr2 = r2_score(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\n\nprint(f\"Views model R²: {r2:.4f}\")\nprint(f\"Views model MSE: {mse:.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Do the same for the <span style=\"font-weight: bold\">Funny</span>, <span style=\"font-weight: bold\">Confusing</span>, and <span style=\"font-weight: bold\">Inspiring</span> ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "X_tags = data[[col for col in data.columns if col.startswith('tag_')]]\n\nmodels_ratings = {}\nfor rating in ['Funny', 'Confusing', 'Inspiring']:\n    y_rating = data[rating]\n    X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_tags, y_rating, test_size=0.2, random_state=42)\n    \n    model = LinearRegression()\n    model.fit(X_train_r, y_train_r)\n    models_ratings[rating] = model\n    \n    y_pred_r = model.predict(X_test_r)\n    r2_r = r2_score(y_test_r, y_pred_r)\n    mse_r = mean_squared_error(y_test_r, y_pred_r)\n    \n    print(f\"{rating} model R²: {r2_r:.4f}, MSE: {mse_r:.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. You will probably notice that most of the tags are not useful in predicting the views and the ratings. You should use some kind of variable selection to prune the set of tags that are included in the model. You can use for example classical p-values or more modern [LASSO](https://en.wikipedia.org/wiki/Lasso_(statistics)) techniques. Which tags are the best predictors of each of the response variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.linear_model import LassoCV\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_tags_scaled = scaler.fit_transform(X_tags)\n\ntargets = {\n    'views': data['views'],\n    'Funny': data['Funny'],\n    'Confusing': data['Confusing'],\n    'Inspiring': data['Inspiring']\n}\n\nimportant_tags = {}\nfor target_name, target_values in targets.items():\n    lasso = LassoCV(cv=5, random_state=42, max_iter=10000)\n    lasso.fit(X_tags_scaled, target_values)\n    \n    coefs = pd.DataFrame({\n        'tag': [col.replace('tag_', '') for col in X_tags.columns],\n        'coefficient': lasso.coef_\n    })\n    coefs['abs_coef'] = np.abs(coefs['coefficient'])\n    important = coefs[coefs['abs_coef'] > 0].sort_values('abs_coef', ascending=False)\n    important_tags[target_name] = important\n    \n    print(f\"\\n{target_name} - Top 10 important tags:\")\n    print(important.head(10)[['tag', 'coefficient']].to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Produce summaries of your results. Could you recommend good tags – or tags to avoid! – for speakers targeting plenty of views and/or certain ratings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"TED Talks 标签使用建议\")\nprint(\"=\" * 60)\n\nfor target_name in ['views', 'Funny', 'Confusing', 'Inspiring']:\n    tags_df = important_tags[target_name]\n    positive_tags = tags_df[tags_df['coefficient'] > 0].head(5)\n    negative_tags = tags_df[tags_df['coefficient'] < 0].head(5)\n    \n    print(f\"\\n【{target_name}】\")\n    if len(positive_tags) > 0:\n        print(f\"  推荐使用的标签: {', '.join(positive_tags['tag'].values)}\")\n    if len(negative_tags) > 0:\n        print(f\"  避免使用的标签: {', '.join(negative_tags['tag'].values)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember to submit your code on the MOOC platform. You can return this Jupyter notebook (.ipynb) or .py, .R, etc depending on your programming preferences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 | Symbol classification (part 2)\n",
    "\n",
    "Note that it is strongly recommended to use Python in this exercise. However, if you can find a suitable AutoML implementation for your favorite language (e.g [here](http://h2o-release.s3.amazonaws.com/h2o/master/3888/docs-website/h2o-docs/automl.html) seems to be one for R) then you are free to use that language as well.\n",
    "\n",
    "Use the preprocessed data from week 3 (you can also produce them using the example solutions of week 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. This time train a *random forest classifier* on the data. A random forest is a collection of *decision trees*, which makes it an *ensemble* of classifiers. Each tree uses a random subset of the features to make its prediction. Without tuning any parameters, how is the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom PIL import Image\nfrom sklearn.ensemble import RandomForestClassifier\n\nlabels_df = pd.read_csv('HASYv2/hasy-data-labels.csv')\nfiltered_labels = labels_df[(labels_df['symbol_id'] >= 70) & (labels_df['symbol_id'] <= 79)]\n\nimages = []\nlabels = []\nfor idx, row in filtered_labels.iterrows():\n    img_path = os.path.join('HASYv2', row['path'])\n    img = Image.open(img_path).convert('L')\n    img_array = np.array(img).flatten()\n    images.append(img_array)\n    labels.append(row['symbol_id'])\n\nX = np.array(images)\ny = np.array(labels)\n\nindices = np.arange(len(X))\nnp.random.seed(42)\nnp.random.shuffle(indices)\nX_shuffled = X[indices]\ny_shuffled = y[indices]\n\nsplit_idx = int(0.8 * len(X_shuffled))\nX_train = X_shuffled[:split_idx]\nX_test = X_shuffled[split_idx:]\ny_train = y_shuffled[:split_idx]\ny_test = y_shuffled[split_idx:]\n\nrf_model = RandomForestClassifier(random_state=42)\nrf_model.fit(X_train, y_train)\n\ntrain_acc = rf_model.score(X_train, y_train)\ntest_acc = rf_model.score(X_test, y_test)\n\nprint(f\"Random Forest - Training accuracy: {train_acc:.4f}\")\nprint(f\"Random Forest - Test accuracy: {test_acc:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The amount of trees to use as a part of the random forest is an example of a hyperparameter, because it is a parameter that is set prior to the learning process. In contrast, a parameter is a value in the model that is learned from the data. Train 20 classifiers, with varying amounts of decision trees starting from 10 up until 200, and plot the test accuracy as a function of the amount of classifiers. Does the accuracy keep increasing? Is more better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\n\nn_estimators_list = np.linspace(10, 200, 20, dtype=int)\ntest_accuracies = []\n\nfor n_est in n_estimators_list:\n    rf = RandomForestClassifier(n_estimators=n_est, random_state=42)\n    rf.fit(X_train, y_train)\n    test_acc = rf.score(X_test, y_test)\n    test_accuracies.append(test_acc)\n    print(f\"n_estimators={n_est}, test accuracy={test_acc:.4f}\")\n\nplt.figure(figsize=(10, 6))\nplt.plot(n_estimators_list, test_accuracies, marker='o')\nplt.xlabel('Number of Trees')\nplt.ylabel('Test Accuracy')\nplt.title('Random Forest: Test Accuracy vs Number of Trees')\nplt.grid(True)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. If we had picked the amount of decision trees by taking the value with the best test accuracy from the last plot, we would have *overfit* our hyperparameters to the test data. Can you see why it is a mistake to tune hyperparameters of your model by using the test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**回答：**\n\n使用测试数据来调整超参数是错误的，原因如下：\n\n1. **数据泄露（Data Leakage）**：如果我们根据测试集的表现来选择超参数，实际上是让模型\"看到\"了测试数据的信息，这违反了测试集应该完全独立的原则。\n\n2. **过拟合到测试集**：当我们反复在测试集上评估不同的超参数配置并选择表现最好的，模型会逐渐\"记住\"测试集的特点，导致在测试集上的性能过于乐观，无法真实反映模型在新数据上的泛化能力。\n\n3. **无法评估真实性能**：测试集的作用是提供对模型泛化能力的无偏估计。一旦用测试集来调参，这个估计就不再无偏，我们就失去了评估模型真实性能的途径。\n\n**正确做法**：应该将数据分为训练集、验证集和测试集三部分。使用训练集训练模型，使用验证集调整超参数，最后使用测试集（仅使用一次）评估最终模型的性能。"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Reshuffle and resplit the data so that it is divided in 3 parts: training (80%), validation (10%) and test (10%). Repeatedly train a model of your choosing (e.g random forest) on the training data, and evaluate it’s performance on the validation set, while tuning the hyperparameters so that the accuracy on the validation set increases. Then, finally evaluate the performance of your model on the test data. What can you say in terms of the generalization of your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "np.random.seed(42)\nindices = np.arange(len(X))\nnp.random.shuffle(indices)\nX_shuffled = X[indices]\ny_shuffled = y[indices]\n\ntrain_end = int(0.8 * len(X_shuffled))\nval_end = int(0.9 * len(X_shuffled))\n\nX_train_new = X_shuffled[:train_end]\ny_train_new = y_shuffled[:train_end]\nX_val = X_shuffled[train_end:val_end]\ny_val = y_shuffled[train_end:val_end]\nX_test_new = X_shuffled[val_end:]\ny_test_new = y_shuffled[val_end:]\n\nprint(f\"Training set: {X_train_new.shape}, Validation set: {X_val.shape}, Test set: {X_test_new.shape}\")\n\nbest_score = 0\nbest_params = {}\n\nfor n_est in [50, 100, 150, 200]:\n    for max_d in [10, 20, 30, None]:\n        rf = RandomForestClassifier(n_estimators=n_est, max_depth=max_d, random_state=42)\n        rf.fit(X_train_new, y_train_new)\n        val_score = rf.score(X_val, y_val)\n        \n        if val_score > best_score:\n            best_score = val_score\n            best_params = {'n_estimators': n_est, 'max_depth': max_d}\n\nprint(f\"\\nBest parameters: {best_params}\")\nprint(f\"Best validation score: {best_score:.4f}\")\n\nfinal_model = RandomForestClassifier(**best_params, random_state=42)\nfinal_model.fit(X_train_new, y_train_new)\n\ntrain_acc_final = final_model.score(X_train_new, y_train_new)\nval_acc_final = final_model.score(X_val, y_val)\ntest_acc_final = final_model.score(X_test_new, y_test_new)\n\nprint(f\"\\nFinal model performance:\")\nprint(f\"Training accuracy: {train_acc_final:.4f}\")\nprint(f\"Validation accuracy: {val_acc_final:.4f}\")\nprint(f\"Test accuracy: {test_acc_final:.4f}\")\n\nprint(f\"\\n泛化能力评估：\")\nprint(f\"训练集和测试集的准确率差异为 {abs(train_acc_final - test_acc_final):.4f}\")\nif abs(train_acc_final - test_acc_final) < 0.05:\n    print(\"模型具有良好的泛化能力，训练集和测试集性能相近。\")\nelif train_acc_final > test_acc_final + 0.1:\n    print(\"模型可能存在过拟合，在训练集上表现明显好于测试集。\")\nelse:\n    print(\"模型泛化能力可接受。\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember to submit your code on the MOOC platform. You can return this Jupyter notebook (.ipynb) or .py, .R, etc depending on your programming preferences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 | TPOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of picking a suitable model, evaluating its performance and tuning the hyperparameters is very time consuming. A new idea in machine learning is the concept of automating this by using an optimization algorithm to find the best model in the space of models and their hyperparameters. Have a look at [TPOT](https://github.com/EpistasisLab/tpot), an automated ML solution that finds a good model and a good set of hyperparameters automatically. Try it on this data, it should outperform simple models like the ones we tried easily. Note that running the algorithm might take a while, depending on the strength of your computer. \n",
    "\n",
    "*Note*: In case it is running for too long, try checking if the parameters you are using when calling TPOT are reasonable, i.e. try reducing number of ‘generations’ or ‘population_size’. TPOT uses cross-validation internally, so we don’t need our own validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from tpot import TPOTClassifier\n\nX_train_tpot = X_shuffled[:train_end]\ny_train_tpot = y_shuffled[:train_end]\nX_test_tpot = X_shuffled[train_end:]\ny_test_tpot = y_shuffled[train_end:]\n\ntpot = TPOTClassifier(\n    generations=5,\n    population_size=20,\n    cv=5,\n    random_state=42,\n    verbosity=2,\n    n_jobs=-1\n)\n\ntpot.fit(X_train_tpot, y_train_tpot)\n\ntpot_score = tpot.score(X_test_tpot, y_test_tpot)\nprint(f\"\\nTPOT Test Accuracy: {tpot_score:.4f}\")\n\ntpot.export('tpot_pipeline.py')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember to submit your code on the MOOC platform. You can return this Jupyter notebook (.ipynb) or .py, .R, etc depending on your programming preferences.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}